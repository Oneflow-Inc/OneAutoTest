{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6366f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp args_libai_bert_init.sh args_libai_bert_loss.sh args_libai_gpt2_init.sh args_libai_gpt2_loss.sh args_libai_t5_init.sh args_libai_t5_loss.sh ./libai/tools/\n",
    "!cp ../bert_nl24_nah16_hs1024.py ../gpt2_nl24_nah16_hs1024.py ../t5_nl12_nah12_hs768.py ./libai/configs/\n",
    "!cp init.sh loss.sh draw_loss.py compose.py ./libai/\n",
    "!cd libai && mkdir loss_txt && mkdir curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c16ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e253f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///home/xuyongning/OneAutoTest/libai/accuracy_verify/libai\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /home/xuyongning/miniconda3/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/setup.py'\"'\"'; __file__='\"'\"'/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-0sif5zk9\n",
      "         cwd: /home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/\n",
      "    Complete output (7 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/setup.py\", line 66, in <module>\n",
      "        include_dirs=[get_pybind11().get_include()],\n",
      "      File \"/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/setup.py\", line 49, in get_pybind11\n",
      "        import pybind11 as pb\n",
      "    ModuleNotFoundError: No module named 'pybind11'\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding file:///home/xuyongning/OneAutoTest/libai/accuracy_verify/libai. Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "+ CONFIG=configs/bert_nl24_nah16_hs1024.py\n",
      "+ NNODES=1\n",
      "+ GPUS_PER_NODE=1\n",
      "+ NODE_RANK=0\n",
      "+ MASTER_ADDR=127.0.0.1\n",
      "+ MASTER_PORT=12345\n",
      "+ MP=1\n",
      "+ PP=1\n",
      "+ USE_FP16=true\n",
      "+ ACTIVATION_CHECKPOINT=true\n",
      "+ MICRO_BATCH_SIZE=32\n",
      "+ GLOBAL_BATCH_SIZE=128\n",
      "+ NUM_LAYER=24\n",
      "+ RUN_COMMIT=master\n",
      "+ TRAIN_ITERS=2\n",
      "+ LOG_PERIOD=1\n",
      "+ TRAN_MODEL=LibAI_bert\n",
      "++ date +%Y%m%d_%H%M%S%N\n",
      "+ RUN_TIME=20220915_090658594787249\n",
      "+ LOG_FOLDER=test_logs_init/master/1n1g\n",
      "+ AMP_OR=FP32\n",
      "+ true\n",
      "+ AMP_OR=FP16\n",
      "+ LOG_FILENAME=test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g\n",
      "+ mkdir -p test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g\n",
      "+ source /home/xuyongning/miniconda3/bin/activate master\n",
      "++ _CONDA_ROOT=/home/xuyongning/miniconda3\n",
      "++ . /home/xuyongning/miniconda3/etc/profile.d/conda.sh\n",
      "+++ export CONDA_EXE=/home/xuyongning/miniconda3/bin/conda\n",
      "+++ CONDA_EXE=/home/xuyongning/miniconda3/bin/conda\n",
      "+++ export _CE_M=\n",
      "+++ _CE_M=\n",
      "+++ export _CE_CONDA=\n",
      "+++ _CE_CONDA=\n",
      "+++ export CONDA_PYTHON_EXE=/home/xuyongning/miniconda3/bin/python\n",
      "+++ CONDA_PYTHON_EXE=/home/xuyongning/miniconda3/bin/python\n",
      "+++ '[' -z x ']'\n",
      "++ conda activate master\n",
      "++ local cmd=activate\n",
      "++ case \"$cmd\" in\n",
      "++ __conda_activate activate master\n",
      "++ '[' -n '' ']'\n",
      "++ local ask_conda\n",
      "+++ PS1=\n",
      "+++ __conda_exe shell.posix activate master\n",
      "+++ __add_sys_prefix_to_path\n",
      "+++ '[' -n '' ']'\n",
      "++++ dirname /home/xuyongning/miniconda3/bin/conda\n",
      "+++ SYSP=/home/xuyongning/miniconda3/bin\n",
      "++++ dirname /home/xuyongning/miniconda3/bin\n",
      "+++ SYSP=/home/xuyongning/miniconda3\n",
      "+++ '[' -n '' ']'\n",
      "+++ PATH=/home/xuyongning/miniconda3/bin:/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "+++ export PATH\n",
      "+++ /home/xuyongning/miniconda3/bin/conda shell.posix activate master\n",
      "++ ask_conda='PS1='\\''(master) '\\''\n",
      "export PATH='\\''/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/envs/master/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\\''\n",
      "export CONDA_PREFIX='\\''/home/xuyongning/miniconda3/envs/master'\\''\n",
      "export CONDA_SHLVL='\\''2'\\''\n",
      "export CONDA_DEFAULT_ENV='\\''master'\\''\n",
      "export CONDA_PROMPT_MODIFIER='\\''(master) '\\''\n",
      "export CONDA_EXE='\\''/home/xuyongning/miniconda3/bin/conda'\\''\n",
      "export _CE_M='\\'''\\''\n",
      "export _CE_CONDA='\\'''\\''\n",
      "export CONDA_PYTHON_EXE='\\''/home/xuyongning/miniconda3/bin/python'\\''\n",
      "export CONDA_PREFIX_1='\\''/home/xuyongning/miniconda3'\\'''\n",
      "++ eval 'PS1='\\''(master) '\\''\n",
      "export PATH='\\''/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/envs/master/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\\''\n",
      "export CONDA_PREFIX='\\''/home/xuyongning/miniconda3/envs/master'\\''\n",
      "export CONDA_SHLVL='\\''2'\\''\n",
      "export CONDA_DEFAULT_ENV='\\''master'\\''\n",
      "export CONDA_PROMPT_MODIFIER='\\''(master) '\\''\n",
      "export CONDA_EXE='\\''/home/xuyongning/miniconda3/bin/conda'\\''\n",
      "export _CE_M='\\'''\\''\n",
      "export _CE_CONDA='\\'''\\''\n",
      "export CONDA_PYTHON_EXE='\\''/home/xuyongning/miniconda3/bin/python'\\''\n",
      "export CONDA_PREFIX_1='\\''/home/xuyongning/miniconda3'\\'''\n",
      "+++ PS1='(master) '\n",
      "+++ export PATH=/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/envs/master/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "+++ PATH=/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/envs/master/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "+++ export CONDA_PREFIX=/home/xuyongning/miniconda3/envs/master\n",
      "+++ CONDA_PREFIX=/home/xuyongning/miniconda3/envs/master\n",
      "+++ export CONDA_SHLVL=2\n",
      "+++ CONDA_SHLVL=2\n",
      "+++ export CONDA_DEFAULT_ENV=master\n",
      "+++ CONDA_DEFAULT_ENV=master\n",
      "+++ export 'CONDA_PROMPT_MODIFIER=(master) '\n",
      "+++ CONDA_PROMPT_MODIFIER='(master) '\n",
      "+++ export CONDA_EXE=/home/xuyongning/miniconda3/bin/conda\n",
      "+++ CONDA_EXE=/home/xuyongning/miniconda3/bin/conda\n",
      "+++ export _CE_M=\n",
      "+++ _CE_M=\n",
      "+++ export _CE_CONDA=\n",
      "+++ _CE_CONDA=\n",
      "+++ export CONDA_PYTHON_EXE=/home/xuyongning/miniconda3/bin/python\n",
      "+++ CONDA_PYTHON_EXE=/home/xuyongning/miniconda3/bin/python\n",
      "+++ export CONDA_PREFIX_1=/home/xuyongning/miniconda3\n",
      "+++ CONDA_PREFIX_1=/home/xuyongning/miniconda3\n",
      "++ __conda_hashr\n",
      "++ '[' -n '' ']'\n",
      "++ '[' -n '' ']'\n",
      "++ hash -r\n",
      "+ pip install -e .\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///home/xuyongning/OneAutoTest/libai/accuracy_verify/libai\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.24.23)\n",
      "Requirement already satisfied: botocore in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.27.23)\n",
      "Requirement already satisfied: cloudpickle in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.1.0)\n",
      "Requirement already satisfied: flowvision==0.1.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (0.1.0)\n",
      "Requirement already satisfied: wget in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (3.2)\n",
      "Requirement already satisfied: hydra-core in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from LiBai==0.2.0) (1.1.2)\n",
      "Requirement already satisfied: nltk in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (3.7)\n",
      "Requirement already satisfied: numpy in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.21.1)\n",
      "Requirement already satisfied: omegaconf==2.1.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.1.0)\n",
      "Requirement already satisfied: Pygments in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.12.0)\n",
      "Requirement already satisfied: PyYAML in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from LiBai==0.2.0) (6.0)\n",
      "Requirement already satisfied: jieba in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (0.42.1)\n",
      "Requirement already satisfied: regex in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2022.6.2)\n",
      "Requirement already satisfied: requests in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from LiBai==0.2.0) (2.28.1)\n",
      "Requirement already satisfied: scipy in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.8.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (0.1.96)\n",
      "Requirement already satisfied: tabulate in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (0.8.10)\n",
      "Requirement already satisfied: termcolor in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (4.64.0)\n",
      "Requirement already satisfied: pybind11 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.9.2)\n",
      "Requirement already satisfied: portalocker in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.4.0)\n",
      "Requirement already satisfied: flake8==3.8.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (3.8.1)\n",
      "Requirement already satisfied: isort==5.10.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (5.10.1)\n",
      "Requirement already satisfied: black==21.4b in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (21.4b0)\n",
      "Requirement already satisfied: autoflake in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.4)\n",
      "Requirement already satisfied: tensorboardX in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.5.1)\n",
      "Requirement already satisfied: pytest in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (7.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click>=7.1.2 in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (8.1.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (0.4.3)\n",
      "Requirement already satisfied: appdirs in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (1.4.4)\n",
      "Requirement already satisfied: toml>=0.10.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (0.10.2)\n",
      "Requirement already satisfied: pathspec<1,>=0.6 in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (0.9.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from flake8==3.8.1->LiBai==0.2.0) (0.6.1)\n",
      "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /home/xuyongning/.local/lib/python3.8/site-packages (from flake8==3.8.1->LiBai==0.2.0) (2.6.0)\n",
      "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from flake8==3.8.1->LiBai==0.2.0) (2.2.0)\n",
      "Requirement already satisfied: rich in /home/xuyongning/.local/lib/python3.8/site-packages (from flowvision==0.1.0->LiBai==0.2.0) (12.4.4)\n",
      "Requirement already satisfied: six in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from flowvision==0.1.0->LiBai==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from flowvision==0.1.0->LiBai==0.2.0) (9.2.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from omegaconf==2.1.0->LiBai==0.2.0) (4.8)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from boto3->LiBai==0.2.0) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from boto3->LiBai==0.2.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from botocore->LiBai==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from botocore->LiBai==0.2.0) (1.26.12)\n",
      "Requirement already satisfied: importlib-resources<5.3 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from hydra-core->LiBai==0.2.0) (5.2.3)\n",
      "Requirement already satisfied: joblib in /home/xuyongning/.local/lib/python3.8/site-packages (from nltk->LiBai==0.2.0) (1.1.0)\n",
      "Requirement already satisfied: packaging in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (21.3)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (1.1.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (22.1.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (2.0.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (1.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from requests->LiBai==0.2.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from requests->LiBai==0.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from requests->LiBai==0.2.0) (2.1.1)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from tensorboardX->LiBai==0.2.0) (3.20.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from importlib-resources<5.3->hydra-core->LiBai==0.2.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/xuyongning/.local/lib/python3.8/site-packages (from packaging->pytest->LiBai==0.2.0) (3.0.9)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from rich->flowvision==0.1.0->LiBai==0.2.0) (0.9.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from rich->flowvision==0.1.0->LiBai==0.2.0) (4.3.0)\n",
      "Installing collected packages: LiBai\n",
      "  Attempting uninstall: LiBai\n",
      "    Found existing installation: LiBai 0.2.0\n",
      "    Uninstalling LiBai-0.2.0:\n",
      "      Successfully uninstalled LiBai-0.2.0\n",
      "  Running setup.py develop for LiBai\n",
      "Successfully installed LiBai\n",
      "+ python3 -m oneflow.distributed.launch --nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr 127.0.0.1 --master_port 12345 tools/train_net.py --config-file configs/bert_nl24_nah16_hs1024.py model.cfg.hidden_layers=24 train.dist.pipeline_num_layers=24 train.train_micro_batch_size=32 train.global_batch_size=128 train.dist.tensor_parallel_size=1 train.dist.pipeline_parallel_size=1 train.amp.enabled=true train.activation_checkpoint.enabled=true train.train_iter=2 train.log_period=1 train.output_dir=test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g\n",
      "+ tee test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g/output.log\n",
      "loaded library: /lib/libibverbs.so.1\n",
      "loaded library: /lib/libibverbs.so.1\n",
      "\u001b[32m[09/15 09:07:03 libai]: \u001b[0mRank of current process: 0. World size: 1\n",
      "\u001b[32m[09/15 09:07:03 libai]: \u001b[0mCommand line arguments: Namespace(config_file='configs/bert_nl24_nah16_hs1024.py', eval_only=False, fast_dev_run=False, opts=['model.cfg.hidden_layers=24', 'train.dist.pipeline_num_layers=24', 'train.train_micro_batch_size=32', 'train.global_batch_size=128', 'train.dist.tensor_parallel_size=1', 'train.dist.pipeline_parallel_size=1', 'train.amp.enabled=true', 'train.activation_checkpoint.enabled=true', 'train.train_iter=2', 'train.log_period=1', 'train.output_dir=test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g'], resume=False)\n",
      "\u001b[32m[09/15 09:07:03 libai]: \u001b[0mContents of args.config_file=configs/bert_nl24_nah16_hs1024.py:\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mlibai\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mconfig\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mLazyCall\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mlibai\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mevaluation\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mPPLEvaluator\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mmodels\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbert\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mpretrain_model\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81mas\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmodel\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mmodels\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mgraph\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mgraph\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mtrain\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15moptim\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15moptim\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mdata\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbert_dataset\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mtokenization\u001b[39m\n",
      "\n",
      "\u001b[38;5;15mvocab_file\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186m/home/xuyongning/libai_dataset/bert-base-chinese-vocab.txt\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\u001b[38;5;15mdata_prefix\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186m/home/xuyongning/libai_dataset/loss_compara_content_sentence\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\n",
      "\u001b[38;5;15mtokenization\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtokenizer\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mvocab_file\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mvocab_file\u001b[39m\n",
      "\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mdataset\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m0\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mdata_prefix\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdata_prefix\u001b[39m\n",
      "\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mdataset\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m0\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mindexed_dataset\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mdata_prefix\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdata_prefix\u001b[39m\n",
      "\u001b[38;5;242m# dataloader.train.num_workers = 4\u001b[39m\n",
      "\n",
      "\u001b[38;5;242m# Bert-large model config\u001b[39m\n",
      "\u001b[38;5;242m#model.cfg.hidden_layers = 24\u001b[39m\n",
      "\u001b[38;5;15mmodel\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mcfg\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mnum_attention_heads\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m16\u001b[39m\n",
      "\u001b[38;5;15mmodel\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mcfg\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mhidden_size\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
      "\n",
      "\u001b[38;5;242m#train.dist.pipeline_num_layers = model.cfg.hidden_layers\u001b[39m\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtest_micro_batch_size\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
      "\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mevaluation\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mevaluator\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mLazyCall\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mPPLEvaluator\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15m)\u001b[39m\n",
      "\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15minput_placement_device\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mcpu\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mevaluation\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15menabled\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81mFalse\u001b[39m\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mevaluation\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15meval_iter\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/15 09:07:03 libai]: \u001b[0mFull config saved to test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g/config.yaml\n",
      "\u001b[32m[09/15 09:07:03 lb.engine.default]: \u001b[0m> compiling dataset index builder ...\n",
      "make: Entering directory '/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/libai/data/data_utils'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/libai/data/data_utils'\n",
      "\u001b[32m[09/15 09:07:03 lb.engine.default]: \u001b[0m>>> done with dataset index builder. Compilation time: 0.060 seconds\n",
      "\u001b[32m[09/15 09:07:03 lb.engine.default]: \u001b[0m>>> done with compiling. Compilation time: 0.061 seconds\n",
      "\u001b[32m[09/15 09:07:04 lb.engine.default]: \u001b[0mPrepare training, validating, testing set\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mbuilding dataset index ...\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mwarming up index mmap file...\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mreading sizes...\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mreading pointers...\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mreading document index...\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mwarming up data mmap file...\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mcreating numpy buffer of mmap...\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mcreating memory view of numpy buffer...\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mFinished creating indexed dataset in 0.101294 seconds\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mindexed dataset stats:\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mnumber of documents: 50000\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.indexed_dataset]: \u001b[0mnumber of sentences: 1249934\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m > loading indexed mapping from /home/xuyongning/libai_dataset/loss_compara_content_sentence_bert_indexmap_256mns_509msl_0.10ssp_1234s.npy\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m    loaded indexed file in 0.004 seconds\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m    total number of samples: 113036\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m > loading indexed mapping from /home/xuyongning/libai_dataset/loss_compara_content_sentence_bert_indexmap_4mns_509msl_0.10ssp_1234s.npy\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m    loaded indexed file in 0.000 seconds\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m    total number of samples: 5884\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m > loading indexed mapping from /home/xuyongning/libai_dataset/loss_compara_content_sentence_bert_indexmap_4mns_509msl_0.10ssp_1234s.npy\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m    loaded indexed file in 0.000 seconds\n",
      "\u001b[32m[09/15 09:07:04 lb.data.data_utils.dataset_utils]: \u001b[0m    total number of samples: 5884\n",
      "W20220915 09:07:04.367767 4127205 env_global_objects_scope.cpp:273] Skip init RDMA because only one process in this group!\n",
      "\u001b[32m[09/15 09:07:04 lb.engine.default]: \u001b[0mAuto-scaling the config to train.train_iter=2, train.warmup_iter=0\n",
      "\u001b[32m[09/15 09:07:14 lb.engine.default]: \u001b[0mModel:\n",
      "BertForPreTraining(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=1024)\n",
      "      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=1024)\n",
      "      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=1024)\n",
      "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (extended_attn_mask): BertExtendedAttnMask()\n",
      "    (encoders): ModuleList(\n",
      "      (0): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (2): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (3): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (4): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (5): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (6): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (7): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (8): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (9): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (10): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (11): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (12): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (13): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (14): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (15): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (16): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (17): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (18): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (19): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (20): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (21): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (22): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "      (23): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=col)\n",
      "      (activation_func): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls_head): BertPreTrainingHeads(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=data)\n",
      "      (activation_func): GELU()\n",
      "      (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (seq_relationship): Linear1D(in_features=1024, out_features=2, bias=True, parallel=data)\n",
      "    (lm_logits): LMLogits()\n",
      "    (loss_func): BertLoss(\n",
      "      (lm_loss): ParallelCrossEntropyLoss()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/15 09:07:14 lb.scheduler.lr_scheduler]: \u001b[0mwarmup iters equals to zero, return CosineLR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/15 09:07:15 lb.engine.trainer]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[09/15 09:07:15 lb.models.utils.graph_base]: \u001b[0mStart compling the train graph which may take some time. Please wait for a moment ...\n",
      "\u001b[32m[09/15 09:08:05 lb.utils.events]: \u001b[0m iteration: 0/2  consumed_samples: 128  total_loss: 10.89  lm_loss: 10.16  sop_loss: 0.7247  data_time: 0.8464 s/iter  lr: 1.00e-04  \n",
      "\u001b[32m[09/15 09:08:11 lb.utils.checkpoint]: \u001b[0mSaving checkpoint to test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g/model_final\n",
      "\u001b[32m[09/15 09:08:18 lb.utils.events]: \u001b[0m eta: 0:00:00  iteration: 1/2  consumed_samples: 256  total_loss: 10.91  lm_loss: 10.16  sop_loss: 0.7473  data_time: 0.4270 s/iter  lr: 5.05e-05  \n",
      "\u001b[32m[09/15 09:08:18 lb.engine.hooks]: \u001b[0mTotal training time: 0:00:07 (0:00:07 on hooks)\n",
      "init.sh: line 67: \n",
      "export CUDA_VISIBLE_DEVICES=0,1,4,5\n",
      "#  1n4g dp4 acc4        bert_nl24_nah16_hs1024_fp16_actrue_mp1_pp1_mb32_gb512_1n4g\n",
      "bash tools/args_libai_bert_init.sh configs/bert_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 1 1 true true 32 512\n",
      "\n",
      "#  1n4g 2-D + acc4  bert_nl24_nah16_hs1024_fp16_actrue_mp2_pp1_mb32_gb256_1n4g\n",
      "bash tools/args_libai_bert_init.sh configs/bert_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 2 1 true true 32 256\n",
      "\n",
      "#  1n4g 2-D + acc4  bert_nl24_nah16_hs1024_fp16_actrue_mp1_pp2_mb32_gb256_1n4g\n",
      "bash tools/args_libai_bert_init.sh configs/bert_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 1 2 true true 32 256\n",
      "\n",
      "#  1n4g 2-D + acc4  bert_nl24_nah16_hs1024_fp16_actrue_mp2_pp2_mb64_gb256_1n4g\n",
      "bash tools/args_libai_bert_init.sh configs/bert_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 2 2 true true 64 256\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "#  1n8g 3-D acc4  bert_nl24_nah16_hs1024_fp16_actrue_mp2_pp2_mb64_gb512_1n8g\n",
      "bash tools/args_libai_bert_init.sh configs/bert_nl24_nah16_hs1024.py 1 8 0 127.0.0.1 2 2 true true 64 512\n",
      "\n",
      "\n",
      "## gpt\n",
      "#  1n1g acc4        gpt2_nl24_nah16_hs1024_fp16_actrue_mp1_pp1_mb8_gb32_1n1g\n",
      "bash tools/args_libai_gpt2_init.sh configs/gpt2_nl24_nah16_hs1024.py 1 1 0 127.0.0.1 1 1 true true 8 32\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,4,5\n",
      "#  1n4g dp4 acc4        gpt2_nl24_nah16_hs1024_fp16_actrue_mp1_pp1_mb4_gb64_1n4g\n",
      "bash tools/args_libai_gpt2_init.sh configs/gpt2_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 1 1 true true 4 64\n",
      "\n",
      "#  1n4g 2-D + acc4  gpt2_nl24_nah16_hs1024_fp16_actrue_mp2_pp1_mb8_gb64_1n4g\n",
      "bash tools/args_libai_gpt2_init.sh configs/gpt2_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 2 1 true true 8 64\n",
      "\n",
      "#  1n4g 2-D + acc4  gpt2_nl24_nah16_hs1024_fp16_actrue_mp1_pp2_mb8_gb64_1n4g\n",
      "bash tools/args_libai_gpt2_init.sh configs/gpt2_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 1 2 true true 8 64\n",
      "\n",
      "#  1n4g 2-D + acc4  gpt2_nl24_nah16_hs1024_fp16_actrue_mp2_pp2_mb16_gb64_1n4g\n",
      "bash tools/args_libai_gpt2_init.sh configs/gpt2_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 2 2 true true 16 64\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "#  1n8g 3-D acc4  gpt2_nl24_nah16_hs1024_fp16_actrue_mp2_pp2_mb16_gb128_1n8g\n",
      "bash tools/args_libai_gpt2_init.sh configs/gpt2_nl24_nah16_hs1024.py 1 8 0 127.0.0.1 2 2 true true 16 128\n",
      "\n",
      "\n",
      "## t5\n",
      "#  1n1g acc4        t5_nl12_nah12_hs768_fp16_actrue_mp1_pp1_mb32_gb128_1n1g\n",
      "bash tools/args_libai_t5_init.sh configs/t5_nl12_nah12_hs768.py 1 1 0 127.0.0.1 1 1 true true 32 128\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,4,5\n",
      "#  1n4g dp4 acc4        t5_nl12_nah12_hs768_fp16_actrue_mp1_pp1_mb16_gb256_1n4g\n",
      "bash tools/args_libai_t5_init.sh configs/t5_nl12_nah12_hs768.py 1 4 0 127.0.0.1 1 1 true true 16 256\n",
      "\n",
      "#  1n4g 2-D + acc4  t5_nl12_nah12_hs768_fp16_actrue_mp2_pp1_mb128_gb1024_1n4g\n",
      "bash tools/args_libai_t5_init.sh configs/t5_nl12_nah12_hs768.py 1 4 0 127.0.0.1 2 1 true true 128 1024\n",
      "\n",
      "#  1n4g 2-D + acc4  t5_nl12_nah12_hs768_fp16_actrue_mp1_pp2_mb32_gb256_1n4g\n",
      "bash tools/args_libai_t5_init.sh configs/t5_nl12_nah12_hs768.py 1 4 0 127.0.0.1 1 2 true true 32 256\n",
      "\n",
      "#  1n4g 2-D + acc4  t5_nl12_nah12_hs768_fp16_actrue_mp2_pp2_mb64_gb256_1n4g\n",
      "bash tools/args_libai_t5_init.sh configs/t5_nl12_nah12_hs768.py 1 4 0 127.0.0.1 2 2 true true 64 256\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "#  1n8g 3-D acc4  t5_nl12_nah12_hs768_fp16_actrue_mp2_pp2_mb64_gb512_1n8g\n",
      "bash tools/args_libai_t5_init.sh configs/t5_nl12_nah12_hs768.py 1 8 0 127.0.0.1 2 2 true true 64 512\n",
      ": No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('./libai')\n",
    "!bash init.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b69e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CONFIG=configs/bert_nl24_nah16_hs1024.py\n",
      "+ NNODES=1\n",
      "+ GPUS_PER_NODE=1\n",
      "+ NODE_RANK=0\n",
      "+ MASTER_ADDR=127.0.0.1\n",
      "+ MASTER_PORT=12345\n",
      "+ MP=1\n",
      "+ PP=1\n",
      "+ USE_FP16=true\n",
      "+ ACTIVATION_CHECKPOINT=true\n",
      "+ MICRO_BATCH_SIZE=32\n",
      "+ GLOBAL_BATCH_SIZE=128\n",
      "+ ACC_COMMIT=3d5e919\n",
      "+ NUM_LAYER=24\n",
      "+ RUN_COMMIT=master\n",
      "+ TRAIN_ITERS=100\n",
      "+ LOG_PERIOD=20\n",
      "+ TRAN_MODEL=LibAI_bert\n",
      "+ WEIGHT_FOLDER=test_logs_init/master/1n1g\n",
      "+ AMP_OR=FP32\n",
      "+ true\n",
      "+ AMP_OR=FP16\n",
      "+ FILENAME=LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g\n",
      "+ WEIGHT_FILENAME=test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g/model_final/\n",
      "+ for TEST_COMMIT in \"master\" ${ACC_COMMIT}\n",
      "+ '[' master == 3d5e919 ']'\n",
      "+ source /home/xuyongning/miniconda3/bin/activate master\n",
      "++ _CONDA_ROOT=/home/xuyongning/miniconda3\n",
      "++ . /home/xuyongning/miniconda3/etc/profile.d/conda.sh\n",
      "+++ export CONDA_EXE=/home/xuyongning/miniconda3/bin/conda\n",
      "+++ CONDA_EXE=/home/xuyongning/miniconda3/bin/conda\n",
      "+++ export _CE_M=\n",
      "+++ _CE_M=\n",
      "+++ export _CE_CONDA=\n",
      "+++ _CE_CONDA=\n",
      "+++ export CONDA_PYTHON_EXE=/home/xuyongning/miniconda3/bin/python\n",
      "+++ CONDA_PYTHON_EXE=/home/xuyongning/miniconda3/bin/python\n",
      "+++ '[' -z x ']'\n",
      "++ conda activate master\n",
      "++ local cmd=activate\n",
      "++ case \"$cmd\" in\n",
      "++ __conda_activate activate master\n",
      "++ '[' -n '' ']'\n",
      "++ local ask_conda\n",
      "+++ PS1=\n",
      "+++ __conda_exe shell.posix activate master\n",
      "+++ __add_sys_prefix_to_path\n",
      "+++ '[' -n '' ']'\n",
      "++++ dirname /home/xuyongning/miniconda3/bin/conda\n",
      "+++ SYSP=/home/xuyongning/miniconda3/bin\n",
      "++++ dirname /home/xuyongning/miniconda3/bin\n",
      "+++ SYSP=/home/xuyongning/miniconda3\n",
      "+++ '[' -n '' ']'\n",
      "+++ PATH=/home/xuyongning/miniconda3/bin:/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "+++ export PATH\n",
      "+++ /home/xuyongning/miniconda3/bin/conda shell.posix activate master\n",
      "++ ask_conda='PS1='\\''(master) '\\''\n",
      "export PATH='\\''/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/envs/master/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\\''\n",
      "export CONDA_PREFIX='\\''/home/xuyongning/miniconda3/envs/master'\\''\n",
      "export CONDA_SHLVL='\\''2'\\''\n",
      "export CONDA_DEFAULT_ENV='\\''master'\\''\n",
      "export CONDA_PROMPT_MODIFIER='\\''(master) '\\''\n",
      "export CONDA_EXE='\\''/home/xuyongning/miniconda3/bin/conda'\\''\n",
      "export _CE_M='\\'''\\''\n",
      "export _CE_CONDA='\\'''\\''\n",
      "export CONDA_PYTHON_EXE='\\''/home/xuyongning/miniconda3/bin/python'\\''\n",
      "export CONDA_PREFIX_1='\\''/home/xuyongning/miniconda3'\\'''\n",
      "++ eval 'PS1='\\''(master) '\\''\n",
      "export PATH='\\''/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/envs/master/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'\\''\n",
      "export CONDA_PREFIX='\\''/home/xuyongning/miniconda3/envs/master'\\''\n",
      "export CONDA_SHLVL='\\''2'\\''\n",
      "export CONDA_DEFAULT_ENV='\\''master'\\''\n",
      "export CONDA_PROMPT_MODIFIER='\\''(master) '\\''\n",
      "export CONDA_EXE='\\''/home/xuyongning/miniconda3/bin/conda'\\''\n",
      "export _CE_M='\\'''\\''\n",
      "export _CE_CONDA='\\'''\\''\n",
      "export CONDA_PYTHON_EXE='\\''/home/xuyongning/miniconda3/bin/python'\\''\n",
      "export CONDA_PREFIX_1='\\''/home/xuyongning/miniconda3'\\'''\n",
      "+++ PS1='(master) '\n",
      "+++ export PATH=/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/envs/master/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "+++ PATH=/home/xuyongning/.local/bin:/home/xuyongning/miniconda3/envs/master/bin:/home/xuyongning/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "+++ export CONDA_PREFIX=/home/xuyongning/miniconda3/envs/master\n",
      "+++ CONDA_PREFIX=/home/xuyongning/miniconda3/envs/master\n",
      "+++ export CONDA_SHLVL=2\n",
      "+++ CONDA_SHLVL=2\n",
      "+++ export CONDA_DEFAULT_ENV=master\n",
      "+++ CONDA_DEFAULT_ENV=master\n",
      "+++ export 'CONDA_PROMPT_MODIFIER=(master) '\n",
      "+++ CONDA_PROMPT_MODIFIER='(master) '\n",
      "+++ export CONDA_EXE=/home/xuyongning/miniconda3/bin/conda\n",
      "+++ CONDA_EXE=/home/xuyongning/miniconda3/bin/conda\n",
      "+++ export _CE_M=\n",
      "+++ _CE_M=\n",
      "+++ export _CE_CONDA=\n",
      "+++ _CE_CONDA=\n",
      "+++ export CONDA_PYTHON_EXE=/home/xuyongning/miniconda3/bin/python\n",
      "+++ CONDA_PYTHON_EXE=/home/xuyongning/miniconda3/bin/python\n",
      "+++ export CONDA_PREFIX_1=/home/xuyongning/miniconda3\n",
      "+++ CONDA_PREFIX_1=/home/xuyongning/miniconda3\n",
      "++ __conda_hashr\n",
      "++ '[' -n '' ']'\n",
      "++ '[' -n '' ']'\n",
      "++ hash -r\n",
      "+ pip install -e .\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///home/xuyongning/OneAutoTest/libai/accuracy_verify/libai\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.24.23)\n",
      "Requirement already satisfied: botocore in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.27.23)\n",
      "Requirement already satisfied: cloudpickle in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.1.0)\n",
      "Requirement already satisfied: flowvision==0.1.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (0.1.0)\n",
      "Requirement already satisfied: wget in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (3.2)\n",
      "Requirement already satisfied: hydra-core in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from LiBai==0.2.0) (1.1.2)\n",
      "Requirement already satisfied: nltk in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (3.7)\n",
      "Requirement already satisfied: numpy in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.21.1)\n",
      "Requirement already satisfied: omegaconf==2.1.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.1.0)\n",
      "Requirement already satisfied: Pygments in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.12.0)\n",
      "Requirement already satisfied: PyYAML in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from LiBai==0.2.0) (6.0)\n",
      "Requirement already satisfied: jieba in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (0.42.1)\n",
      "Requirement already satisfied: regex in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2022.6.2)\n",
      "Requirement already satisfied: requests in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from LiBai==0.2.0) (2.28.1)\n",
      "Requirement already satisfied: scipy in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.8.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (0.1.96)\n",
      "Requirement already satisfied: tabulate in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (0.8.10)\n",
      "Requirement already satisfied: termcolor in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (4.64.0)\n",
      "Requirement already satisfied: pybind11 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.9.2)\n",
      "Requirement already satisfied: portalocker in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.4.0)\n",
      "Requirement already satisfied: flake8==3.8.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (3.8.1)\n",
      "Requirement already satisfied: isort==5.10.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (5.10.1)\n",
      "Requirement already satisfied: black==21.4b in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (21.4b0)\n",
      "Requirement already satisfied: autoflake in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (1.4)\n",
      "Requirement already satisfied: tensorboardX in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (2.5.1)\n",
      "Requirement already satisfied: pytest in /home/xuyongning/.local/lib/python3.8/site-packages (from LiBai==0.2.0) (7.1.2)\n",
      "Requirement already satisfied: toml>=0.10.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (0.10.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (0.4.3)\n",
      "Requirement already satisfied: click>=7.1.2 in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (8.1.3)\n",
      "Requirement already satisfied: pathspec<1,>=0.6 in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (0.9.0)\n",
      "Requirement already satisfied: appdirs in /home/xuyongning/.local/lib/python3.8/site-packages (from black==21.4b->LiBai==0.2.0) (1.4.4)\n",
      "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from flake8==3.8.1->LiBai==0.2.0) (2.2.0)\n",
      "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /home/xuyongning/.local/lib/python3.8/site-packages (from flake8==3.8.1->LiBai==0.2.0) (2.6.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from flake8==3.8.1->LiBai==0.2.0) (0.6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from flowvision==0.1.0->LiBai==0.2.0) (9.2.0)\n",
      "Requirement already satisfied: six in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from flowvision==0.1.0->LiBai==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: rich in /home/xuyongning/.local/lib/python3.8/site-packages (from flowvision==0.1.0->LiBai==0.2.0) (12.4.4)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from omegaconf==2.1.0->LiBai==0.2.0) (4.8)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from boto3->LiBai==0.2.0) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from boto3->LiBai==0.2.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/xuyongning/.local/lib/python3.8/site-packages (from botocore->LiBai==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from botocore->LiBai==0.2.0) (1.26.12)\n",
      "Requirement already satisfied: importlib-resources<5.3 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from hydra-core->LiBai==0.2.0) (5.2.3)\n",
      "Requirement already satisfied: joblib in /home/xuyongning/.local/lib/python3.8/site-packages (from nltk->LiBai==0.2.0) (1.1.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (1.11.0)\n",
      "Requirement already satisfied: packaging in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (21.3)\n",
      "Requirement already satisfied: iniconfig in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (2.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from pytest->LiBai==0.2.0) (22.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from requests->LiBai==0.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from requests->LiBai==0.2.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages (from requests->LiBai==0.2.0) (2022.6.15)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from tensorboardX->LiBai==0.2.0) (3.20.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from importlib-resources<5.3->hydra-core->LiBai==0.2.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/xuyongning/.local/lib/python3.8/site-packages (from packaging->pytest->LiBai==0.2.0) (3.0.9)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from rich->flowvision==0.1.0->LiBai==0.2.0) (0.9.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/xuyongning/.local/lib/python3.8/site-packages (from rich->flowvision==0.1.0->LiBai==0.2.0) (4.3.0)\n",
      "Installing collected packages: LiBai\n",
      "  Attempting uninstall: LiBai\n",
      "    Found existing installation: LiBai 0.2.0\n",
      "    Uninstalling LiBai-0.2.0:\n",
      "      Successfully uninstalled LiBai-0.2.0\n",
      "  Running setup.py develop for LiBai\n",
      "Successfully installed LiBai\n",
      "+ sed -i s#loss2.txt#loss_txt/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_loss_master.txt#g draw_loss.py\n",
      "+ sed -i s#loss2#LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_loss_master#g draw_loss.py\n",
      "+ sed -i s#your_loss#LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_loss_master#g libai/engine/trainer.py\n",
      "++ date +%Y%m%d_%H%M%S%N\n",
      "+ RUN_TIME=20220915_091002050991040\n",
      "+ LOG_FOLDER=test_logs_loss/master/1n1g\n",
      "+ LOG_FILENAME=test_logs_loss/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_20220915_091002050991040\n",
      "+ mkdir -p test_logs_loss/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_20220915_091002050991040\n",
      "+ python3 -m oneflow --doctor\n",
      "loaded library: /lib/libibverbs.so.1\n",
      "path: ['/home/xuyongning/miniconda3/envs/master/lib/python3.8/site-packages/oneflow']\n",
      "version: 0.8.1+cu112.git.ecafd61\n",
      "git_commit: ecafd61\n",
      "cmake_build_type: Release\n",
      "rdma: True\n",
      "mlir: True\n",
      "+ tee test_logs_loss/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_20220915_091002050991040/output.log\n",
      "+ python3 -m oneflow.distributed.launch --nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr 127.0.0.1 --master_port 12345 tools/train_net.py --config-file configs/bert_nl24_nah16_hs1024.py model.cfg.hidden_dropout_prob=0.0 model.cfg.attention_probs_dropout_prob=0.0 model.cfg.bias_dropout_fusion=False model.cfg.hidden_layers=24 train.load_weight=test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g/model_final/ train.dist.pipeline_num_layers=24 train.train_micro_batch_size=32 train.global_batch_size=128 train.dist.tensor_parallel_size=1 train.dist.pipeline_parallel_size=1 train.amp.enabled=true train.activation_checkpoint.enabled=true train.train_iter=100 train.log_period=20 train.output_dir=test_logs_loss/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_20220915_091002050991040\n",
      "loaded library: /lib/libibverbs.so.1\n",
      "loaded library: /lib/libibverbs.so.1\n",
      "\u001b[32m[09/15 09:10:05 libai]: \u001b[0mRank of current process: 0. World size: 1\n",
      "\u001b[32m[09/15 09:10:05 libai]: \u001b[0mCommand line arguments: Namespace(config_file='configs/bert_nl24_nah16_hs1024.py', eval_only=False, fast_dev_run=False, opts=['model.cfg.hidden_dropout_prob=0.0', 'model.cfg.attention_probs_dropout_prob=0.0', 'model.cfg.bias_dropout_fusion=False', 'model.cfg.hidden_layers=24', 'train.load_weight=test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g/model_final/', 'train.dist.pipeline_num_layers=24', 'train.train_micro_batch_size=32', 'train.global_batch_size=128', 'train.dist.tensor_parallel_size=1', 'train.dist.pipeline_parallel_size=1', 'train.amp.enabled=true', 'train.activation_checkpoint.enabled=true', 'train.train_iter=100', 'train.log_period=20', 'train.output_dir=test_logs_loss/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_20220915_091002050991040'], resume=False)\n",
      "\u001b[32m[09/15 09:10:05 libai]: \u001b[0mContents of args.config_file=configs/bert_nl24_nah16_hs1024.py:\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mlibai\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mconfig\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mLazyCall\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mlibai\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mevaluation\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mPPLEvaluator\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mmodels\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbert\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mpretrain_model\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81mas\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmodel\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mmodels\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mgraph\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mgraph\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mtrain\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15moptim\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15moptim\u001b[39m\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mdata\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbert_dataset\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mtokenization\u001b[39m\n",
      "\n",
      "\u001b[38;5;15mvocab_file\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186m/home/xuyongning/libai_dataset/bert-base-chinese-vocab.txt\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\u001b[38;5;15mdata_prefix\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186m/home/xuyongning/libai_dataset/loss_compara_content_sentence\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\n",
      "\u001b[38;5;15mtokenization\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtokenizer\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mvocab_file\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mvocab_file\u001b[39m\n",
      "\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mdataset\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m0\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mdata_prefix\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdata_prefix\u001b[39m\n",
      "\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mdataset\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m0\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mindexed_dataset\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mdata_prefix\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdata_prefix\u001b[39m\n",
      "\u001b[38;5;242m# dataloader.train.num_workers = 4\u001b[39m\n",
      "\n",
      "\u001b[38;5;242m# Bert-large model config\u001b[39m\n",
      "\u001b[38;5;242m#model.cfg.hidden_layers = 24\u001b[39m\n",
      "\u001b[38;5;15mmodel\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mcfg\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mnum_attention_heads\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m16\u001b[39m\n",
      "\u001b[38;5;15mmodel\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mcfg\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mhidden_size\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
      "\n",
      "\u001b[38;5;242m#train.dist.pipeline_num_layers = model.cfg.hidden_layers\u001b[39m\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtest_micro_batch_size\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
      "\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mevaluation\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mevaluator\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mLazyCall\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mPPLEvaluator\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15m)\u001b[39m\n",
      "\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15minput_placement_device\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mcpu\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mevaluation\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15menabled\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81mFalse\u001b[39m\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mevaluation\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15meval_iter\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30\u001b[39m\n",
      "\n",
      "\u001b[32m[09/15 09:10:05 libai]: \u001b[0mFull config saved to test_logs_loss/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g_20220915_091002050991040/config.yaml\n",
      "\u001b[32m[09/15 09:10:05 lb.engine.default]: \u001b[0m> compiling dataset index builder ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/libai/data/data_utils'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/xuyongning/OneAutoTest/libai/accuracy_verify/libai/libai/data/data_utils'\n",
      "\u001b[32m[09/15 09:10:05 lb.engine.default]: \u001b[0m>>> done with dataset index builder. Compilation time: 0.056 seconds\n",
      "\u001b[32m[09/15 09:10:05 lb.engine.default]: \u001b[0m>>> done with compiling. Compilation time: 0.057 seconds\n",
      "\u001b[32m[09/15 09:10:05 lb.engine.default]: \u001b[0mPrepare training, validating, testing set\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mbuilding dataset index ...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mwarming up index mmap file...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mreading sizes...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mreading pointers...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mreading document index...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mwarming up data mmap file...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mcreating numpy buffer of mmap...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mcreating memory view of numpy buffer...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mFinished creating indexed dataset in 0.096088 seconds\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mindexed dataset stats:\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mnumber of documents: 50000\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.indexed_dataset]: \u001b[0mnumber of sentences: 1249934\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m > WARNING: could not find index map file /home/xuyongning/libai_dataset/loss_compara_content_sentence_bert_indexmap_12800mns_509msl_0.10ssp_1234s.npy, building the indices on rank 0 ...\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m > building samples index mapping for bert ...\n",
      "    using uint32 for data mapping...\n",
      "    using:\n",
      "     number of documents:            47450\n",
      "     sentences range:                [0, 1188464)\n",
      "     total number of sentences:      1188464\n",
      "     number of epochs:               2147483646\n",
      "     maximum number of samples:      12800\n",
      "     maximum sequence length:        509\n",
      "     short sequence probability:     0.1\n",
      "     short sequence ration (1/prob): 10\n",
      "     seed:                           1234\n",
      "    reached 12800 samples after 1 epochs ...\n",
      "   number of empty documents: 0\n",
      "   number of documents with one sentence: 711\n",
      "   number of documents with long sentences: 2092\n",
      "   will create mapping for 113036 samples\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m > done building samples index maping\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m > saved the index mapping in /home/xuyongning/libai_dataset/loss_compara_content_sentence_bert_indexmap_12800mns_509msl_0.10ssp_1234s.npy\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m > elapsed time to build and save samples mapping (seconds): 0.019636\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m > loading indexed mapping from /home/xuyongning/libai_dataset/loss_compara_content_sentence_bert_indexmap_12800mns_509msl_0.10ssp_1234s.npy\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m    loaded indexed file in 0.004 seconds\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m    total number of samples: 113036\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m > loading indexed mapping from /home/xuyongning/libai_dataset/loss_compara_content_sentence_bert_indexmap_4mns_509msl_0.10ssp_1234s.npy\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m    loaded indexed file in 0.000 seconds\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m    total number of samples: 5884\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m > loading indexed mapping from /home/xuyongning/libai_dataset/loss_compara_content_sentence_bert_indexmap_4mns_509msl_0.10ssp_1234s.npy\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m    loaded indexed file in 0.000 seconds\n",
      "\u001b[32m[09/15 09:10:05 lb.data.data_utils.dataset_utils]: \u001b[0m    total number of samples: 5884\n",
      "W20220915 09:10:06.036613 4141394 env_global_objects_scope.cpp:273] Skip init RDMA because only one process in this group!\n",
      "\u001b[32m[09/15 09:10:06 lb.engine.default]: \u001b[0mAuto-scaling the config to train.train_iter=100, train.warmup_iter=0\n",
      "\u001b[32m[09/15 09:10:16 lb.engine.default]: \u001b[0mModel:\n",
      "BertForPreTraining(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=1024)\n",
      "      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=1024)\n",
      "      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=1024)\n",
      "      (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (extended_attn_mask): BertExtendedAttnMask()\n",
      "    (encoders): ModuleList(\n",
      "      (0): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (12): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (13): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (14): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (15): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (16): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (17): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (18): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (19): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (20): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (21): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (22): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (23): TransformerLayer(\n",
      "        (drop_path): Identity()\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          hidden_size=1024, num_heads=16, is_cross_attention=False\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (query_key_value): Linear1D(in_features=1024, out_features=3072, bias=True, parallel=col)\n",
      "          (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=row)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          bias_gelu_fusion=True, bias_dropout_fusion=False, dropout=0.0\n",
      "          (dense_h_to_4h): Linear1D(in_features=1024, out_features=4096, bias=True, parallel=col)\n",
      "          (dense_4h_to_h): Linear1D(in_features=4096, out_features=1024, bias=True, parallel=row)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=col)\n",
      "      (activation_func): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls_head): BertPreTrainingHeads(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (dense): Linear1D(in_features=1024, out_features=1024, bias=True, parallel=data)\n",
      "      (activation_func): GELU()\n",
      "      (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (seq_relationship): Linear1D(in_features=1024, out_features=2, bias=True, parallel=data)\n",
      "    (lm_logits): LMLogits()\n",
      "    (loss_func): BertLoss(\n",
      "      (lm_loss): ParallelCrossEntropyLoss()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/15 09:10:16 lb.scheduler.lr_scheduler]: \u001b[0mwarmup iters equals to zero, return CosineLR\n",
      "\u001b[32m[09/15 09:10:16 lb.utils.checkpoint]: \u001b[0mLoading checkpoint from test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g/model_final/\n",
      "\u001b[32m[09/15 09:10:21 lb.utils.checkpoint]: \u001b[0miter info in test_logs_init/master/1n1g/LibAI_bert_nl24_nah16_hs1024_FP16_actrue_mp1_pp1_mb32_gb128_1n1g/model_final/ not found, set iter to 0\n",
      "\u001b[32m[09/15 09:10:22 lb.engine.trainer]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[09/15 09:10:22 lb.models.utils.graph_base]: \u001b[0mStart compling the train graph which may take some time. Please wait for a moment ...\n",
      "^C\n",
      "loss.sh: line 68: \n",
      "export CUDA_VISIBLE_DEVICES=0,1,4,5\n",
      "#  1n4g dp4 acc4        bert_nl24_nah16_hs1024_fp16_actrue_mp1_pp1_mb32_gb512_1n4g\n",
      "bash tools/args_libai_bert_loss.sh configs/bert_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 1 1 true true 32 512 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  bert_nl24_nah16_hs1024_fp16_actrue_mp2_pp1_mb32_gb256_1n4g\n",
      "bash tools/args_libai_bert_loss.sh configs/bert_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 2 1 true true 32 256 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  bert_nl24_nah16_hs1024_fp16_actrue_mp1_pp2_mb32_gb256_1n4g\n",
      "bash tools/args_libai_bert_loss.sh configs/bert_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 1 2 true true 32 256 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  bert_nl24_nah16_hs1024_fp16_actrue_mp2_pp2_mb64_gb256_1n4g\n",
      "bash tools/args_libai_bert_loss.sh configs/bert_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 2 2 true true 64 256 ${ACC_COMMIT}\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "#  1n8g 3-D acc4  bert_nl24_nah16_hs1024_fp16_actrue_mp2_pp2_mb64_gb512_1n8g\n",
      "bash tools/args_libai_bert_loss.sh configs/bert_nl24_nah16_hs1024.py 1 8 0 127.0.0.1 2 2 true true 64 512 ${ACC_COMMIT}\n",
      "\n",
      "\n",
      "## gpt\n",
      "#  1n1g acc4        gpt2_nl24_nah16_hs1024_fp16_actrue_mp1_pp1_mb8_gb32_1n1g\n",
      "#bash tools/args_libai_gpt2_loss.sh configs/gpt2_nl24_nah16_hs1024.py 1 1 0 127.0.0.1 1 1 true true 8 32 ${ACC_COMMIT}\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,4,5\n",
      "#  1n4g dp4 acc4        gpt2_nl24_nah16_hs1024_fp16_actrue_mp1_pp1_mb4_gb64_1n4g\n",
      "bash tools/args_libai_gpt2_loss.sh configs/gpt2_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 1 1 true true 4 64 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  gpt2_nl24_nah16_hs1024_fp16_actrue_mp2_pp1_mb8_gb64_1n4g\n",
      "bash tools/args_libai_gpt2_loss.sh configs/gpt2_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 2 1 true true 8 64 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  gpt2_nl24_nah16_hs1024_fp16_actrue_mp1_pp2_mb8_gb64_1n4g\n",
      "#bash tools/args_libai_gpt2_loss.sh configs/gpt2_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 1 2 true true 8 64 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  gpt2_nl24_nah16_hs1024_fp16_actrue_mp2_pp2_mb16_gb64_1n4g\n",
      "bash tools/args_libai_gpt2_loss.sh configs/gpt2_nl24_nah16_hs1024.py 1 4 0 127.0.0.1 2 2 true true 16 64 ${ACC_COMMIT}\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "#  1n8g 3-D acc4  gpt2_nl24_nah16_hs1024_fp16_actrue_mp2_pp2_mb16_gb128_1n8g\n",
      "bash tools/args_libai_gpt2_loss.sh configs/gpt2_nl24_nah16_hs1024.py 1 8 0 127.0.0.1 2 2 true true 16 128 ${ACC_COMMIT}\n",
      "\n",
      "\n",
      "## t5\n",
      "#  1n1g acc4        t5_nl12_nah12_hs768_fp16_actrue_mp1_pp1_mb32_gb128_1n1g\n",
      "bash tools/args_libai_t5_loss.sh configs/t5_nl12_nah12_hs768.py 1 1 0 127.0.0.1 1 1 true true 32 128 ${ACC_COMMIT}\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,4,5\n",
      "#  1n4g dp4 acc4        t5_nl12_nah12_hs768_fp16_actrue_mp1_pp1_mb16_gb256_1n4g\n",
      "bash tools/args_libai_t5_loss.sh configs/t5_nl12_nah12_hs768.py 1 4 0 127.0.0.1 1 1 true true 16 256 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  t5_nl12_nah12_hs768_fp16_actrue_mp2_pp1_mb128_gb1024_1n4g\n",
      "bash tools/args_libai_t5_loss.sh configs/t5_nl12_nah12_hs768.py 1 4 0 127.0.0.1 2 1 true true 128 1024 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  t5_nl12_nah12_hs768_fp16_actrue_mp1_pp2_mb32_gb256_1n4g\n",
      "bash tools/args_libai_t5_loss.sh configs/t5_nl12_nah12_hs768.py 1 4 0 127.0.0.1 1 2 true true 32 256 ${ACC_COMMIT}\n",
      "\n",
      "#  1n4g 2-D + acc4  t5_nl12_nah12_hs768_fp16_actrue_mp2_pp2_mb64_gb256_1n4g\n",
      "bash tools/args_libai_t5_loss.sh configs/t5_nl12_nah12_hs768.py 1 4 0 127.0.0.1 2 2 true true 64 256 ${ACC_COMMIT}\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "#  1n8g 3-D acc4  t5_nl12_nah12_hs768_fp16_actrue_mp2_pp2_mb64_gb512_1n8g\n",
      "bash tools/args_libai_t5_loss.sh configs/t5_nl12_nah12_hs768.py 1 8 0 127.0.0.1 2 2 true true 64 512 ${ACC_COMMIT}\n",
      ": No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash loss.sh \"3d5e919\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
