set -ex

export ONEFLOW_COMM_NET_IB_GID_INDEX=$NCCL_IB_GID_INDEX
export ONEFLOW_COMM_NET_IB_HCA=mlx5_2:1

# volcengine DDP env
NNODES=$MLP_WORKER_NUM
GPUS_PER_NODE=$MLP_WORKER_GPU
NODE_RANK=$MLP_ROLE_INDEX

MASTER_ADDR=$MLP_WORKER_0_HOST
MASTER_PORT=$MLP_WORKER_0_PORT

CONFIG=$1
MP=${2:-1}
PP=${3:-1}
GRAPH_ENABLED=${4:-true}
USE_FP16=${5:-true}
ACTIVATION_CHECKPOINT=${6:-false}
MICRO_BATCH_SIZE=${7:-4}
ACC=${8:-1}
ZERO_ENABLE=${9:-false}
ZERO_STAGE=${10:-2}
TRAIN_ITERS=${11:-220}
LOG_PERIOD=${12:-100}
NUM_LAYER=${13:-12}
NUM_ATT_HEADS=${14:-12}
HIDDEN_SIZE=${15:-768}
INTERMEDIATE_SIZE=${16:-3072}
HEAD_SIZE=${17:-64}
SAVE_MODEL=${18:-false}
UNSET_DROPOUT=${19:-false}

DP=$(expr $NNODES \* $GPUS_PER_NODE \/ $MP \/ $PP)
GLOBAL_BATCH_SIZE=$((ACC * DP * MICRO_BATCH_SIZE))

source args_train.sh $CONFIG $NNODES $GPUS_PER_NODE $NODE_RANK $MASTER_ADDR $MASTER_PORT $MP $PP $GRAPH_ENABLED $USE_FP16 $ACTIVATION_CHECKPOINT $MICRO_BATCH_SIZE $GLOBAL_BATCH_SIZE $ZERO_ENABLE $ZERO_STAGE $TRAIN_ITERS $LOG_PERIOD $NUM_LAYER $NUM_ATT_HEADS $HIDDEN_SIZE $INTERMEDIATE_SIZE $HEAD_SIZE $SAVE_MODEL $UNSET_DROPOUT
