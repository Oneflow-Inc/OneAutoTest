set -ex

export NCCL_IB_DISABLE=0
export NCCL_DEBUG=INFO
export NCCL_IB_GID_INDEX=3
export NCCL_GDR_LEVEL=2
# 安装 TCCL 之后不需要 NCCL TOPO 文件 comment 这行
# export NCCL_TOPO_FILE=/data_32T/home/workspace/nccl-tests/nccl_topo_a800_1.6t.xml
export NCCL_IB_QPS_PER_CONNECTION=4
export ONEFLOW_COMM_NET_IB_GID_INDEX=3
#export ONEFLOW_COMM_NET_IB_HCA=$NCCL_IB_HCA
export ONEFLOW_COMM_NET_IB_HCA=mlx5_bond_1:1

CONFIG=$1
NNODES=${2:-1}
GPUS_PER_NODE=${3:-8}
NODE_RANK=${4:-0}
MASTER_ADDR=${5:-"127.0.0.1"}
MASTER_PORT=12345
MP=${6:-1}
PP=${7:-1}
GRAPH_ENABLED=${8:-true}
USE_FP16=${9:-true}
ACTIVATION_CHECKPOINT=${10:-false}
MICRO_BATCH_SIZE=${11:-4}
ACC=${12:-1}
ZERO_ENABLE=${13:-false}
ZERO_STAGE=${14:-2}
TRAIN_ITERS=${15:-220}
LOG_PERIOD=${16:-100}
NUM_LAYER=${17:-12}
NUM_ATT_HEADS=${18:-12}
HIDDEN_SIZE=${19:-768}
INTERMEDIATE_SIZE=${20:-3072}
HEAD_SIZE=${21:-64}
SAVE_MODEL=${22:-false}
UNSET_DROPOUT=${23:-false}

DP=$(expr $NNODES \* $GPUS_PER_NODE \/ $MP \/ $PP)
GLOBAL_BATCH_SIZE=$((ACC * DP * MICRO_BATCH_SIZE))

source args_train.sh $CONFIG $NNODES $GPUS_PER_NODE $NODE_RANK $MASTER_ADDR $MASTER_PORT $MP $PP $GRAPH_ENABLED $USE_FP16 $ACTIVATION_CHECKPOINT $MICRO_BATCH_SIZE $GLOBAL_BATCH_SIZE $ZERO_ENABLE $ZERO_STAGE $TRAIN_ITERS $LOG_PERIOD $NUM_LAYER $NUM_ATT_HEADS $HIDDEN_SIZE $INTERMEDIATE_SIZE $HEAD_SIZE $SAVE_MODEL $UNSET_DROPOUT
